{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (2.0.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: jax in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (0.4.28)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (0.4.28)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (3.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (1.24.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (4.10.0.82)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jax->mediapipe) (1.11.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\franf\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\franf\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\franf\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.10.0.82)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opencv-python) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\franf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe\n",
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Cargar la imagen\n",
    "image = cv2.imread('../yolo/videoframe_0.jpg')\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Realizar la detección de pose\n",
    "results = pose.process(image_rgb)\n",
    "\n",
    "blazepose_results = []\n",
    "\n",
    "# Dibujar las anotaciones de la pose en la imagen\n",
    "if results.pose_landmarks:\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    #guardar en annotations\n",
    "    keypoints = []\n",
    "    for landmark in results.pose_landmarks.landmark:\n",
    "        keypoints.extend([landmark.x * image.shape[1], landmark.y * image.shape[0], landmark.visibility])\n",
    "    \n",
    "    ann = {\n",
    "        \"image_id\": \"videoframe\",\n",
    "        \"category_id\": 1,  # Persona\n",
    "        \"keypoints\": keypoints,\n",
    "        \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "    }\n",
    "    blazepose_results.append(ann)\n",
    "\n",
    "#guardar la img\n",
    "cv2.imwrite('testBlazePose.jpg', image)\n",
    "# Mostrar la imagen con las poses detectadas\n",
    "cv2.imshow('BlazePose Result', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Liberar recursos\n",
    "pose.close()\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usando el dataset de wholeBody\n",
    "import os\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Directorio de imágenes y archivo de anotaciones\n",
    "image_dir = '../../val2017/val2017'  # Ajusta según la estructura de tu directorio\n",
    "#annotation_file = '../../coco_wholebody_val_v1.0.json'\n",
    "annotation_file = '../../person_keypoints_val2017.json'\n",
    "blazepose_results = []\n",
    "\n",
    "# Cargar el dataset COCO\n",
    "with open(annotation_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Procesar las imágenes\n",
    "for img_info in coco_data['images']:\n",
    "    image_path = os.path.join(image_dir, img_info['file_name'])\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        keypoints = []\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.extend([landmark.x * image.shape[1], landmark.y * image.shape[0], landmark.visibility])\n",
    "        \n",
    "        ann = {\n",
    "            \"image_id\": img_info['id'],\n",
    "            \"category_id\": 1,  # Persona\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "pose.close()\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results-wholeBody.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n",
    "\n",
    "#guardar una img para ver como la analiza blazepose\n",
    "cv2.imwrite('testBlazePoseWholeBody.jpg', image) #seria la ultima imagen procesada del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n",
      "DONE (t=0.86s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.088\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.164\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.087\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.170\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.132\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.201\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.039\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.258\n",
      "mAP:  0.08849632371629948\n",
      "AP@0.50:  0.16401605305236133\n",
      "AP@0.75:  0.08710477466870943\n",
      "AP (M):  0.02910787421689213\n",
      "AP (L):  0.17029027758930437\n",
      "AR:  0.03933351543294182\n",
      "Anotaciones guardadas en: blazepose_results-wholeBody.json\n",
      "Imagen procesada guardada en: testBlazePoseWholeBody.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Directorio de imágenes y archivo de anotaciones\n",
    "image_dir = '../../val2017/val2017'  # Ajusta según la estructura de tu directorio\n",
    "annotation_file = '../yolo/datasets/coco-pose/annotations/person_keypoints_val2017.json'\n",
    "blazepose_results = []\n",
    "\n",
    "# Cargar el dataset COCO\n",
    "with open(annotation_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Función para convertir 33 keypoints de BlazePose a 17 keypoints de COCO\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 17\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = landmarks[1]  # Left eye\n",
    "    keypoints[2] = landmarks[4]  # Right eye\n",
    "    keypoints[3] = landmarks[7]  # Left ear\n",
    "    keypoints[4] = landmarks[8]  # Right ear\n",
    "    keypoints[5] = landmarks[11]  # Left shoulder\n",
    "    keypoints[6] = landmarks[12]  # Right shoulder\n",
    "    keypoints[7] = landmarks[13]  # Left elbow\n",
    "    keypoints[8] = landmarks[14]  # Right elbow\n",
    "    keypoints[9] = landmarks[15]  # Left wrist\n",
    "    keypoints[10] = landmarks[16]  # Right wrist\n",
    "    keypoints[11] = landmarks[23]  # Left hip\n",
    "    keypoints[12] = landmarks[24]  # Right hip\n",
    "    keypoints[13] = landmarks[25]  # Left knee\n",
    "    keypoints[14] = landmarks[26]  # Right knee\n",
    "    keypoints[15] = landmarks[27]  # Left ankle\n",
    "    keypoints[16] = landmarks[28]  # Right ankle\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])  # Fill with zeros if no point detected\n",
    "    return coco_keypoints\n",
    "\n",
    "# Procesar las imágenes\n",
    "for img_info in coco_data['images']:\n",
    "    image_path = os.path.join(image_dir, img_info['file_name'])\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Dibujar las anotaciones de la pose en la imagen\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, image.shape)\n",
    "        \n",
    "        ann = {\n",
    "            \"image_id\": img_info['id'],\n",
    "            \"category_id\": 1,  # Persona\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results-wholeBody.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n",
    "\n",
    "# Guardar y mostrar una imagen procesada\n",
    "if 'image' in locals():\n",
    "    # Guardar la última imagen procesada con las anotaciones\n",
    "    cv2.imwrite('testBlazePoseWholeBody.jpg', image)\n",
    "    # Mostrar la imagen con las poses detectadas\n",
    "    cv2.imshow('BlazePose WholeBody Result', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Liberar recursos\n",
    "pose.close()\n",
    "\n",
    "# Evaluar las métricas de rendimiento usando pycocotools\n",
    "coco_dt = coco.loadRes('blazepose_results-wholeBody.json')\n",
    "coco_eval = COCOeval(coco, coco_dt, 'keypoints')\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "# Imprimir métricas relevantes\n",
    "print(\"mAP: \", coco_eval.stats[0])\n",
    "print(\"AP@0.50: \", coco_eval.stats[1])\n",
    "print(\"AP@0.75: \", coco_eval.stats[2])\n",
    "print(\"AP (M): \", coco_eval.stats[3])\n",
    "print(\"AP (L): \", coco_eval.stats[4])\n",
    "print(\"AR: \", coco_eval.stats[8])\n",
    "\n",
    "print(\"Anotaciones guardadas en: blazepose_results-wholeBody.json\")\n",
    "if 'image' in locals():\n",
    "    print(\"Imagen procesada guardada en: testBlazePoseWholeBody.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (25,) (17,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m coco_dt \u001b[38;5;241m=\u001b[39m coco\u001b[38;5;241m.\u001b[39mloadRes(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblazepose_results-wholeBody_v2.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    119\u001b[0m coco_eval \u001b[38;5;241m=\u001b[39m COCOeval(coco, coco_dt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[43mcoco_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m coco_eval\u001b[38;5;241m.\u001b[39maccumulate()\n\u001b[0;32m    122\u001b[0m coco_eval\u001b[38;5;241m.\u001b[39msummarize()\n",
      "File \u001b[1;32mc:\\Users\\franf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pycocotools\\cocoeval.py:148\u001b[0m, in \u001b[0;36mCOCOeval.evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m p\u001b[38;5;241m.\u001b[39miouType \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    147\u001b[0m     computeIoU \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputeOks\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mious \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatId\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputeIoU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatId\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgId\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgIds\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcatId\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcatIds\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    152\u001b[0m evaluateImg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluateImg\n\u001b[0;32m    153\u001b[0m maxDet \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmaxDets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\franf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pycocotools\\cocoeval.py:148\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m p\u001b[38;5;241m.\u001b[39miouType \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    147\u001b[0m     computeIoU \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputeOks\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mious \u001b[38;5;241m=\u001b[39m {(imgId, catId): \u001b[43mcomputeIoU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatId\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m    149\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m imgId \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mimgIds\n\u001b[0;32m    150\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m catId \u001b[38;5;129;01min\u001b[39;00m catIds}\n\u001b[0;32m    152\u001b[0m evaluateImg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluateImg\n\u001b[0;32m    153\u001b[0m maxDet \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmaxDets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\franf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pycocotools\\cocoeval.py:222\u001b[0m, in \u001b[0;36mCOCOeval.computeOks\u001b[1;34m(self, imgId, catId)\u001b[0m\n\u001b[0;32m    219\u001b[0m xd \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m3\u001b[39m]; yd \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k1\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# measure the per-keypoint distance if keypoints visible\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     dx \u001b[38;5;241m=\u001b[39m \u001b[43mxd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxg\u001b[49m\n\u001b[0;32m    223\u001b[0m     dy \u001b[38;5;241m=\u001b[39m yd \u001b[38;5;241m-\u001b[39m yg\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# measure minimum distance to keypoints in (x0,y0) & (x1,y1)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (25,) (17,) "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Directorio de imágenes y archivo de anotaciones\n",
    "image_dir = '../../COCO/val2017/val2017'  # Ajusta según la estructura de tu directorio\n",
    "annotation_file = '../../person_keypoints_val2017.json'\n",
    "blazepose_results = []\n",
    "\n",
    "# Cargar el dataset COCO\n",
    "with open(annotation_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "def average_landmarks_three(landmark1, landmark2, landmark3):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x + landmark3.x) / 3,\n",
    "        'y': (landmark1.y + landmark2.y + landmark3.y) / 3,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility + landmark3.visibility) / 3\n",
    "    }\n",
    "\n",
    "def average_landmarks(landmark1, landmark2):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x) / 2,\n",
    "        'y': (landmark1.y + landmark2.y) / 2,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility) / 2\n",
    "    }\n",
    "\n",
    "# Función para convertir 33 keypoints de BlazePose a 25 keypoints de COCO\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 25\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = average_landmarks(landmarks[12], landmarks[11])  # Neck\n",
    "    keypoints[2] = landmarks[12]  # L shoulder\n",
    "    keypoints[3] = landmarks[14]  # L elbow \n",
    "    keypoints[4] = landmarks[16]  # L hand\n",
    "    keypoints[5] = landmarks[11]  # R shoulder\n",
    "    keypoints[6] = landmarks[13]  # R elbow\n",
    "    keypoints[7] = landmarks[15]  # R hand\n",
    "    keypoints[8] = average_landmarks(landmarks[24], landmarks[23])  # Hip central pelvis\n",
    "    keypoints[9] = landmarks[24]  # L hip\n",
    "    keypoints[10] = landmarks[26]  # L knee\n",
    "    keypoints[11] = landmarks[28]  # L ankle\n",
    "    keypoints[12] = landmarks[23]  # R hip\n",
    "    keypoints[13] = landmarks[25]  # R knee\n",
    "    keypoints[14] = landmarks[27]  # R ankle\n",
    "    keypoints[15] = average_landmarks_three(landmarks[5], landmarks[6], landmarks[4])  # Average of points\n",
    "    keypoints[16] = average_landmarks_three(landmarks[1], landmarks[2], landmarks[3])  # Average of points\n",
    "    keypoints[17] = landmarks[8]  # Custom point\n",
    "    keypoints[18] = landmarks[7]  # Custom point\n",
    "    keypoints[19] = landmarks[29]  # Custom point\n",
    "    keypoints[20] = landmarks[30]  # Custom point\n",
    "    keypoints[21] = landmarks[31]  # Custom point\n",
    "    keypoints[22] = landmarks[32]  # Custom point\n",
    "    keypoints[23] = landmarks[9]  # Custom point\n",
    "    keypoints[24] = landmarks[10]  # Custom point\n",
    "\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            if isinstance(point, dict):\n",
    "                coco_keypoints.extend([point['x'] * image_shape[1], point['y'] * image_shape[0], point['visibility']])\n",
    "            else:\n",
    "                coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])\n",
    "    return coco_keypoints\n",
    "\n",
    "# Procesar las imágenes\n",
    "for img_info in coco_data['images']:\n",
    "    image_path = os.path.join(image_dir, img_info['file_name'])\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Dibujar las anotaciones de la pose en la imagen\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, image.shape)\n",
    "        \n",
    "        ann = {\n",
    "            \"image_id\": img_info['id'],\n",
    "            \"category_id\": 1,  # Persona\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results-wholeBody_v2.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n",
    "\n",
    "# Guardar y mostrar una imagen procesada\n",
    "if 'image' in locals():\n",
    "    # Guardar la última imagen procesada con las anotaciones\n",
    "    cv2.imwrite('testBlazePoseWholeBody.jpg', image)\n",
    "    # Mostrar la imagen con las poses detectadas\n",
    "    cv2.imshow('BlazePose WholeBody Result', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Liberar recursos\n",
    "pose.close()\n",
    "\n",
    "# Evaluar las métricas de rendimiento usando pycocotools\n",
    "coco_dt = coco.loadRes('blazepose_results-wholeBody_v2.json')\n",
    "coco_eval = COCOeval(coco, coco_dt, 'keypoints')\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "# Imprimir métricas relevantes\n",
    "print(\"mAP: \", coco_eval.stats[0])\n",
    "print(\"AP@0.50: \", coco_eval.stats[1])\n",
    "print(\"AP@0.75: \", coco_eval.stats[2])\n",
    "print(\"AP (M): \", coco_eval.stats[3])\n",
    "print(\"AP (L): \", coco_eval.stats[4])\n",
    "print(\"AR: \", coco_eval.stats[8])\n",
    "\n",
    "print(\"Anotaciones guardadas en: blazepose_results-wholeBody_v2.json\")\n",
    "if 'image' in locals():\n",
    "    print(\"Imagen procesada guardada en: testBlazePoseWholeBodyv2.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define area to mark (e.g., a bounding box)\n",
    "rectangles = [\n",
    "    (541, 604, 587, 630),  # Coordenadas de los círculos en el piso\n",
    "    (171, 525, 218, 550),\n",
    "    (162, 390, 197, 408),\n",
    "    (923, 373, 953, 389),\n",
    "    (905, 504, 946, 526),\n",
    "    (768, 299, 793, 311),\n",
    "    (338, 305, 367, 319),\n",
    "    (553, 381, 585, 396),\n",
    "    (559, 279, 582, 289)\n",
    "]\n",
    "\n",
    "def average_landmarks_three(landmark1, landmark2, landmark3):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x + landmark3.x) / 3,\n",
    "        'y': (landmark1.y + landmark2.y + landmark3.y) / 3,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility + landmark3.visibility) / 3\n",
    "    }\n",
    "\n",
    "def average_landmarks(landmark1, landmark2):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x) / 2,\n",
    "        'y': (landmark1.y + landmark2.y) / 2,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility) / 2\n",
    "    }\n",
    "\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 25\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = average_landmarks(landmarks[12], landmarks[11])  # Neck\n",
    "    keypoints[2] = landmarks[12]  # L shoulder\n",
    "    keypoints[3] = landmarks[14]  # L elbow \n",
    "    keypoints[4] = landmarks[16]  # L hand\n",
    "    keypoints[5] = landmarks[11]  # R shoulder\n",
    "    keypoints[6] = landmarks[13]  # R elbow\n",
    "    keypoints[7] = landmarks[15]  # R hand\n",
    "    keypoints[8] = average_landmarks(landmarks[24], landmarks[23])  # Hip central pelvis\n",
    "    keypoints[9] = landmarks[24]  # L hip\n",
    "    keypoints[10] = landmarks[26]  # L knee\n",
    "    keypoints[11] = landmarks[28]  # L ankle\n",
    "    keypoints[12] = landmarks[23]  # R hip\n",
    "    keypoints[13] = landmarks[25]  # R knee\n",
    "    keypoints[14] = landmarks[27]  # R ankle\n",
    "    keypoints[15] = average_landmarks_three(landmarks[5], landmarks[6], landmarks[4])  # Average of points\n",
    "    keypoints[16] = average_landmarks_three(landmarks[1], landmarks[2], landmarks[3])  # Average of points\n",
    "    keypoints[17] = landmarks[8]  # Custom point\n",
    "    keypoints[18] = landmarks[7]  # Custom point\n",
    "    keypoints[19] = landmarks[29]  # Custom point\n",
    "    keypoints[21] = landmarks[31]  # Custom point\n",
    "    keypoints[22] = landmarks[30]  # Custom point\n",
    "    keypoints[24] = landmarks[32]  # Custom point\n",
    "\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            if isinstance(point, dict):\n",
    "                coco_keypoints.extend([point['x'] * image_shape[1], point['y'] * image_shape[0], point['visibility']])\n",
    "            else:\n",
    "                coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])\n",
    "    return coco_keypoints\n",
    "\n",
    "def draw_coco_keypoints(image, keypoints):\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x, y, v = keypoints[i:i+3]\n",
    "        if v > 0:\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "            cv2.putText(image, str(i//3), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "def draw_coco_skeleton(image, keypoints, pairs):\n",
    "    for (start, end) in pairs:\n",
    "        x1, y1, v1 = keypoints[start*3:start*3+3]\n",
    "        x2, y2, v2 = keypoints[end*3:end*3+3]\n",
    "        if v1 > 0 and v2 > 0:\n",
    "            cv2.line(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "def point_in_rectangle(point, rect):\n",
    "    x, y = point\n",
    "    x1, y1, x2, y2 = rect\n",
    "    return x1 <= x <= x2 and y1 <= y <= y2\n",
    "\n",
    "\n",
    "# Cargar el video\n",
    "video_url = 'https://mcp-wildsense.s3.us-east-2.amazonaws.com/videos/7/2024-03-15/11_28_22-player9.mp4'\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "#cap = cv2.VideoCapture('./caminar2.mp4')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('outputFinal.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "blazepose_results = []\n",
    "previous_left_ankle = None\n",
    "previous_right_ankle = None\n",
    "steps = 0\n",
    "step_threshold = 25  # Umbral de distancia para considerar un paso (ajusta según sea necesario)\n",
    "still_threshold = 20  # Umbral de distancia para considerar que la persona está quieta\n",
    "still_frames = 0\n",
    "still_frames_threshold = 10  # Número de cuadros consecutivos para considerar que la persona está quieta\n",
    "is_still = False\n",
    "movement_direction = None\n",
    "movement_threshold = 10  # Umbral de movimiento en el eje X para considerar desplazamiento lateral\n",
    "distance_direction = None\n",
    "distance_threshold = 6  # Umbral de movimiento en el eje Y para considerar acercamiento/alejamiento\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    for rect in rectangles:\n",
    "        cv2.rectangle(frame, (rect[0], rect[1]), (rect[2], rect[3]), (255, 0, 0), 2)  # Draw rectangles\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        \n",
    "        keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, frame.shape)\n",
    "        \n",
    "        draw_coco_keypoints(frame, keypoints)\n",
    "        skeleton_pairs = [\n",
    "            (0, 15), (15, 17), (0, 16), (16, 18), # Cabeza \n",
    "            (0, 1), #cuello\n",
    "            (1, 2), (2, 3), (3,4), #Brazo izquierdo\n",
    "            (1, 5), (6, 7), (5,6), # Brazos derechos\n",
    "            (1,8), #torso\n",
    "            (8, 9), (9, 10), (10, 11), (11, 22), (22, 24),  # Pierna izquierda\n",
    "            (8, 12), (12, 13), (13, 14), (14, 19), (19,21)  # Pierna derecha\n",
    "        ]\n",
    "        draw_coco_skeleton(frame, keypoints, skeleton_pairs)\n",
    "\n",
    "        #left_ankle = np.array([keypoints[35], keypoints[36]])  # Coordenadas del tobillo izquierdo\n",
    "        #right_ankle = np.array([keypoints[39], keypoints[40]])  # Coordenadas del tobillo derecho\n",
    "    \n",
    "    #con este funciona mejor\n",
    "        #left_ankle = np.array([keypoints[33], keypoints[34]])  # Coordenadas del tobillo izquierdo\n",
    "        #right_ankle = np.array([keypoints[39], keypoints[40]])  # Coordenadas del tobillo derecho\n",
    "\n",
    "        left_ankle = np.array([keypoints[11*3], keypoints[11*3+1]])  # Coordenadas del tobillo izquierdo\n",
    "        right_ankle = np.array([keypoints[14*3], keypoints[14*3+1]])   # Coordenadas del tobillo derecho\n",
    "\n",
    "        tip_toe_left = np.array([keypoints[24*3], keypoints[24*3+1]])\n",
    "        tip_toe_right = np.array([keypoints[21*3], keypoints[21*3+1]])\n",
    "\n",
    "        heel_left = np.array([keypoints[22*3], keypoints[22*3+1]])\n",
    "        heel_right = np.array([keypoints[19*3], keypoints[19*3+1]])\n",
    "\n",
    "        # Promedio de puntos\n",
    "        average_left_foot = (left_ankle + tip_toe_left + heel_left) / 3\n",
    "        average_right_foot = (right_ankle + tip_toe_right + heel_right) / 3\n",
    "\n",
    "        #pintar el punto promedio del pie\n",
    "        average_left_foot_paint = average_left_foot.astype(int)\n",
    "        average_right_foot_paint = average_right_foot.astype(int)\n",
    "\n",
    "        cv2.circle(frame, tuple(average_left_foot_paint), 5, (51, 255, 252), -1)\n",
    "        cv2.circle(frame, tuple(average_right_foot_paint), 5, (51, 255, 252), -1)\n",
    "\n",
    "        for rect in rectangles:\n",
    "            if point_in_rectangle(average_left_foot, rect):\n",
    "                cv2.putText(frame, 'Left Ankle in Area', (rect[0], rect[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            if point_in_rectangle(average_right_foot, rect):\n",
    "                cv2.putText(frame, 'Right Ankle in Area', (rect[0], rect[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if previous_left_ankle is not None and previous_right_ankle is not None:\n",
    "            left_distance = np.linalg.norm(average_left_foot - previous_left_ankle)\n",
    "            right_distance = np.linalg.norm(average_right_foot - previous_right_ankle)\n",
    "            if left_distance > step_threshold or right_distance > step_threshold:\n",
    "                steps += 1\n",
    "\n",
    "            if left_distance < still_threshold and right_distance < still_threshold:\n",
    "                still_frames += 1\n",
    "            else:\n",
    "                still_frames = 0\n",
    "\n",
    "            is_still = still_frames >= still_frames_threshold\n",
    "\n",
    "            left_movement_x = average_left_foot[0] - previous_left_ankle[0]\n",
    "            right_movement_x = average_right_foot[0] - previous_right_ankle[0]\n",
    "\n",
    "            if abs(left_movement_x) > movement_threshold or abs(right_movement_x) > movement_threshold:\n",
    "                if left_movement_x > 0 and right_movement_x > 0:\n",
    "                    movement_direction = \"Right\"\n",
    "                elif left_movement_x < 0 and right_movement_x < 0:\n",
    "                    movement_direction = \"Left\"\n",
    "                else:\n",
    "                    movement_direction = \"Unknown\"\n",
    "            else:\n",
    "                movement_direction = \"Still\"\n",
    "\n",
    "            left_movement_y = average_left_foot[1] - previous_left_ankle[1]\n",
    "            right_movement_y = average_right_foot[1] - previous_right_ankle[1]\n",
    "\n",
    "            if abs(left_movement_y) > distance_threshold or abs(right_movement_y) > distance_threshold:\n",
    "                if left_movement_y > 0 and right_movement_y > 0:\n",
    "                    distance_direction = \"Closer\"\n",
    "                    steps += 1\n",
    "                elif left_movement_y < 0 and right_movement_y < 0:\n",
    "                    distance_direction = \"Farther\"\n",
    "                    steps += 1\n",
    "                else:\n",
    "                    distance_direction = \"Unknown\"\n",
    "            else:\n",
    "                distance_direction = \"Stationary\"\n",
    "\n",
    "        previous_left_ankle = average_left_foot\n",
    "        previous_right_ankle = average_right_foot\n",
    "\n",
    "        ann = {\n",
    "            \"image_id\": \"videoframe\",\n",
    "            \"category_id\": 1,\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "    #status_text = f\"Steps: {steps} - Still: {'Yes' if is_still else 'No'} - Moving: {movement_direction} - Distance: {distance_direction}\"\n",
    "    status_text = f\"Still: {'Yes' if is_still else 'No'} - Moving: {movement_direction} - Distance: {distance_direction}\"\n",
    "\n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    out.write(frame)\n",
    "    cv2.imshow('BlazePose Result', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()\n",
    "\n",
    "with open('blazepose_results_finalDetection.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagen de videoframe pero con las anotaciones de coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Cargar la imagen\n",
    "image = cv2.imread('../yolo/videoframe_0.jpg')\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Realizar la detección de pose\n",
    "results = pose.process(image_rgb)\n",
    "blazepose_results = []\n",
    "\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 17\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = landmarks[1]  # Left eye\n",
    "    keypoints[2] = landmarks[4]  # Right eye\n",
    "    keypoints[3] = landmarks[7]  # Left ear\n",
    "    keypoints[4] = landmarks[8]  # Right ear\n",
    "    keypoints[5] = landmarks[11]  # Left shoulder\n",
    "    keypoints[6] = landmarks[12]  # Right shoulder\n",
    "    keypoints[7] = landmarks[13]  # Left elbow\n",
    "    keypoints[8] = landmarks[14]  # Right elbow\n",
    "    keypoints[9] = landmarks[15]  # Left wrist\n",
    "    keypoints[10] = landmarks[16]  # Right wrist\n",
    "    keypoints[11] = landmarks[23]  # Left hip\n",
    "    keypoints[12] = landmarks[24]  # Right hip\n",
    "    keypoints[13] = landmarks[25]  # Left knee\n",
    "    keypoints[14] = landmarks[26]  # Right knee\n",
    "    keypoints[15] = landmarks[27]  # Left ankle\n",
    "    keypoints[16] = landmarks[28]  # Right ankle\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])  # Fill with zeros if no point detected\n",
    "    return coco_keypoints\n",
    "\n",
    "# Función para dibujar puntos clave en la imagen\n",
    "def draw_coco_keypoints(image, keypoints):\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x, y, v = keypoints[i:i+3]\n",
    "        if v > 0:\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1) #pinto de color verde los que son coco\n",
    "\n",
    "\n",
    "# Dibujar las anotaciones de la pose en la imagen\n",
    "if results.pose_landmarks:\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    #convertir a coco keypoints\n",
    "\n",
    "    #guardar en annotations\n",
    "    keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, image.shape)\n",
    "    #dibujarla\n",
    "    draw_coco_keypoints(image, keypoints)\n",
    "    \n",
    "    ann = {\n",
    "        \"image_id\": \"videoframe\",\n",
    "        \"category_id\": 1,  # Persona\n",
    "        \"keypoints\": keypoints,\n",
    "        \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "    }\n",
    "    blazepose_results.append(ann)\n",
    "\n",
    "#guardar la img\n",
    "cv2.imwrite('testBlazePose.jpg', image)\n",
    "# Mostrar la imagen con las poses detectadas\n",
    "cv2.imshow('BlazePose Result', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Liberar recursos\n",
    "pose.close()\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #con video \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Función para convertir puntos clave de BlazePose a COCO\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 17\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = landmarks[1]  # Left eye\n",
    "    keypoints[2] = landmarks[4]  # Right eye\n",
    "    keypoints[3] = landmarks[7]  # Left ear\n",
    "    keypoints[4] = landmarks[8]  # Right ear\n",
    "    keypoints[5] = landmarks[11]  # Left shoulder\n",
    "    keypoints[6] = landmarks[12]  # Right shoulder\n",
    "    keypoints[7] = landmarks[13]  # Left elbow\n",
    "    keypoints[8] = landmarks[14]  # Right elbow\n",
    "    keypoints[9] = landmarks[15]  # Left wrist\n",
    "    keypoints[10] = landmarks[16]  # Right wrist\n",
    "    keypoints[11] = landmarks[23]  # Left hip\n",
    "    keypoints[12] = landmarks[24]  # Right hip\n",
    "    keypoints[13] = landmarks[25]  # Left knee\n",
    "    keypoints[14] = landmarks[26]  # Right knee\n",
    "    keypoints[15] = landmarks[27]  # Left ankle\n",
    "    keypoints[16] = landmarks[28]  # Right ankle\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])  # Fill with zeros if no point detected\n",
    "    return coco_keypoints\n",
    "\n",
    "# Función para dibujar puntos clave en la imagen\n",
    "def draw_coco_keypoints(image, keypoints):\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x, y, v = keypoints[i:i+3]\n",
    "        if v > 0:\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "# Cargar el video\n",
    "cap = cv2.VideoCapture('./caminata.mp4') #cambiar ruta\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "blazepose_results = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Convertir a COCO keypoints\n",
    "        keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, frame.shape)\n",
    "        # Dibujar los puntos clave en formato COCO\n",
    "        draw_coco_keypoints(frame, keypoints)\n",
    "\n",
    "        ann = {\n",
    "            \"image_id\": \"videoframe\",\n",
    "            \"category_id\": 1,  # Persona\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "    # Escribir el cuadro procesado en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Mostrar el cuadro procesado\n",
    "    cv2.imshow('BlazePose Result', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results_video.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Función para convertir puntos clave de BlazePose a COCO\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 17\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = landmarks[1]  # Left eye\n",
    "    keypoints[2] = landmarks[4]  # Right eye\n",
    "    keypoints[3] = landmarks[7]  # Left ear\n",
    "    keypoints[4] = landmarks[8]  # Right ear\n",
    "    keypoints[5] = landmarks[11]  # Left shoulder\n",
    "    keypoints[6] = landmarks[12]  # Right shoulder\n",
    "    keypoints[7] = landmarks[13]  # Left elbow\n",
    "    keypoints[8] = landmarks[14]  # Right elbow\n",
    "    keypoints[9] = landmarks[15]  # Left wrist\n",
    "    keypoints[10] = landmarks[16]  # Right wrist\n",
    "    keypoints[11] = landmarks[23]  # Left hip\n",
    "    keypoints[12] = landmarks[24]  # Right hip\n",
    "    keypoints[13] = landmarks[25]  # Left knee\n",
    "    keypoints[14] = landmarks[26]  # Right knee\n",
    "    keypoints[15] = landmarks[27]  # Left ankle\n",
    "    keypoints[16] = landmarks[28]  # Right ankle\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])  # Fill with zeros if no point detected\n",
    "    return coco_keypoints\n",
    "\n",
    "# Función para dibujar puntos clave en la imagen\n",
    "def draw_coco_keypoints(image, keypoints):\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x, y, v = keypoints[i:i+3]\n",
    "        if v > 0:\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "# Cargar el video\n",
    "video_url = 'https://mcp-wildsense.s3.us-east-2.amazonaws.com/videos/7/2024-03-15/11_28_22-player9.mp4'\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output1.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "blazepose_results = []\n",
    "previous_left_ankle = None\n",
    "previous_right_ankle = None\n",
    "steps = 0\n",
    "step_threshold = 30  # Umbral de distancia para considerar un paso (ajusta según sea necesario)\n",
    "still_threshold = 10  # Umbral de distancia para considerar que la persona está quieta\n",
    "still_frames = 0\n",
    "still_frames_threshold = 20  # Número de cuadros consecutivos para considerar que la persona está quieta\n",
    "is_still = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Convertir a COCO keypoints\n",
    "        keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, frame.shape)\n",
    "        # Dibujar los puntos clave en formato COCO\n",
    "        draw_coco_keypoints(frame, keypoints)\n",
    "\n",
    "        # Obtener las posiciones de los tobillos\n",
    "        left_ankle = np.array([keypoints[45], keypoints[46]])  # Coordenadas del tobillo izquierdo\n",
    "        right_ankle = np.array([keypoints[48], keypoints[49]])  # Coordenadas del tobillo derecho\n",
    "\n",
    "        # Detectar pasos\n",
    "        if previous_left_ankle is not None and previous_right_ankle is not None:\n",
    "            left_distance = np.linalg.norm(left_ankle - previous_left_ankle)\n",
    "            right_distance = np.linalg.norm(right_ankle - previous_right_ankle)\n",
    "            if left_distance > step_threshold or right_distance > step_threshold:\n",
    "                steps += 1\n",
    "\n",
    "            # Detectar si la persona está quieta\n",
    "            if left_distance < still_threshold and right_distance < still_threshold:\n",
    "                still_frames += 1\n",
    "            else:\n",
    "                still_frames = 0\n",
    "\n",
    "            is_still = still_frames >= still_frames_threshold\n",
    "\n",
    "        previous_left_ankle = left_ankle\n",
    "        previous_right_ankle = right_ankle\n",
    "\n",
    "        ann = {\n",
    "            \"image_id\": \"videoframe\",\n",
    "            \"category_id\": 1,  # Persona\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "    # Escribir el cuadro procesado en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Mostrar el cuadro procesado y el contador de pasos y estado de quietud\n",
    "    status_text = f\"Steps: {steps} - Still: {'Yes' if is_still else 'No'}\"\n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('BlazePose Result', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results1.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codigo que usa 3 puntos en el pie: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def average_landmarks_three(landmark1, landmark2, landmark3):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x + landmark3.x) / 3,\n",
    "        'y': (landmark1.y + landmark2.y + landmark3.y) / 3,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility + landmark3.visibility) / 3\n",
    "    }\n",
    "def average_landmarks(landmark1, landmark2):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x) / 2,\n",
    "        'y': (landmark1.y + landmark2.y) / 2,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility) / 2\n",
    "    }\n",
    "# Función para convertir puntos clave de BlazePose a OpenPose\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    \n",
    "    keypoints = [None] * 25  # Cambié a 25 para incluir todos los puntos mencionados\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = average_landmarks(landmarks[12], landmarks[11])  # Neck\n",
    "    keypoints[2] = landmarks[12]  # L shoulder\n",
    "    keypoints[3] = landmarks[14]  # L elbow \n",
    "    keypoints[4] = landmarks[16]  # L hand\n",
    "    keypoints[5] = landmarks[11]  # R shoulder\n",
    "    keypoints[6] = landmarks[13]  # R elbow\n",
    "    keypoints[7] = landmarks[15]  # R hand\n",
    "    keypoints[8] = average_landmarks(landmarks[24], landmarks[23])  # Hip central pelvis\n",
    "    keypoints[9] = landmarks[24]  # L hip\n",
    "    keypoints[10] = landmarks[26]  # L knee\n",
    "    keypoints[11] = landmarks[28]  # L ankle\n",
    "    keypoints[12] = landmarks[23]  # R hip\n",
    "    keypoints[13] = landmarks[25]  # R knee\n",
    "    keypoints[14] = landmarks[27]  # R ankle\n",
    "    keypoints[15] = average_landmarks_three(landmarks[5], landmarks[6], landmarks[4])  # Average of points\n",
    "    keypoints[16] = average_landmarks_three(landmarks[1], landmarks[2], landmarks[3])  # Average of points\n",
    "    keypoints[17] = landmarks[8]  # Custom point\n",
    "    keypoints[18] = landmarks[7]  # Custom point\n",
    "    keypoints[19] = landmarks[29]  # Custom point\n",
    "    keypoints[21] = landmarks[31]  # Custom point\n",
    "    keypoints[22] = landmarks[30]  # Custom point\n",
    "    keypoints[24] = landmarks[32]  # Custom point\n",
    "\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            if isinstance(point, dict):\n",
    "                coco_keypoints.extend([point['x'] * image_shape[1], point['y'] * image_shape[0], point['visibility']])\n",
    "            else:\n",
    "                coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])  # Fill with zeros if no point detected\n",
    "    #print(coco_keypoints)\n",
    "    return coco_keypoints\n",
    "\n",
    "# Función para dibujar puntos clave en la imagen\n",
    "def draw_coco_keypoints(image, keypoints):\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x, y, v = keypoints[i:i+3]\n",
    "        if v > 0:\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "# Cargar el video\n",
    "video_url = 'https://mcp-wildsense.s3.us-east-2.amazonaws.com/videos/7/2024-03-15/11_28_22-player9.mp4'\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "#cap = cv2.VideoCapture('./caminar2.mp4')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "blazepose_results = []\n",
    "previous_left_ankle = None\n",
    "previous_right_ankle = None\n",
    "steps = 0\n",
    "step_threshold = 50  # Umbral de distancia para considerar un paso (ajusta según sea necesario)\n",
    "still_threshold = 40  # Umbral de distancia para considerar que la persona está quieta\n",
    "still_frames = 0\n",
    "still_frames_threshold = 15  # Número de cuadros consecutivos para considerar que la persona está quieta\n",
    "is_still = False\n",
    "movement_direction = None\n",
    "movement_threshold = 10  # Umbral de movimiento en el eje X para considerar desplazamiento lateral\n",
    "distance_direction = None\n",
    "distance_threshold = 5  # Umbral de movimiento en el eje Y para considerar acercamiento/alejamiento\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Convertir a COCO keypoints\n",
    "        keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, frame.shape)\n",
    "        # Dibujar los puntos clave en formato COCO\n",
    "        draw_coco_keypoints(frame, keypoints)\n",
    "\n",
    "        # Obtener las posiciones de los tobillos\n",
    "        left_ankle = np.array([keypoints[33], keypoints[34]])  # Coordenadas del tobillo izquierdo\n",
    "        right_ankle = np.array([keypoints[39], keypoints[40]])  # Coordenadas del tobillo derecho\n",
    "\n",
    "        # Detectar pasos\n",
    "        if previous_left_ankle is not None and previous_right_ankle is not None:\n",
    "            left_distance = np.linalg.norm(left_ankle - previous_left_ankle)\n",
    "            right_distance = np.linalg.norm(right_ankle - previous_right_ankle)\n",
    "            if left_distance > step_threshold or right_distance > step_threshold:\n",
    "                steps += 1\n",
    "\n",
    "            # Detectar si la persona está quieta\n",
    "            if left_distance < still_threshold and right_distance < still_threshold:\n",
    "                still_frames += 1\n",
    "            else:\n",
    "                still_frames = 0\n",
    "\n",
    "            is_still = still_frames >= still_frames_threshold\n",
    "\n",
    "            # Detectar movimiento lateral\n",
    "            left_movement_x = left_ankle[0] - previous_left_ankle[0]\n",
    "            right_movement_x = right_ankle[0] - previous_right_ankle[0]\n",
    "\n",
    "            if abs(left_movement_x) > movement_threshold or abs(right_movement_x) > movement_threshold:\n",
    "                if left_movement_x > 0 and right_movement_x > 0:\n",
    "                    movement_direction = \"Right\"\n",
    "                elif left_movement_x < 0 and right_movement_x < 0:\n",
    "                    movement_direction = \"Left\"\n",
    "                else:\n",
    "                    movement_direction = \"Unknown\"\n",
    "            else:\n",
    "                movement_direction = \"Still\"\n",
    "\n",
    "            # Detectar acercamiento/alejamiento\n",
    "            left_movement_y = left_ankle[1] - previous_left_ankle[1]\n",
    "            right_movement_y = right_ankle[1] - previous_right_ankle[1]\n",
    "\n",
    "            if abs(left_movement_y) > distance_threshold or abs(right_movement_y) > distance_threshold:\n",
    "                if left_movement_y > 0 and right_movement_y > 0:\n",
    "                    distance_direction = \"Closer\"  # acerca\n",
    "                    steps += 1\n",
    "                elif left_movement_y < 0 and right_movement_y < 0:\n",
    "                    distance_direction = \"Farther\"  # aleja\n",
    "                    steps += 1\n",
    "                else:\n",
    "                    distance_direction = \"Unknown\"\n",
    "            else:\n",
    "                distance_direction = \"Stationary\"  # quiero en eje y\n",
    "\n",
    "        previous_left_ankle = left_ankle\n",
    "        previous_right_ankle = right_ankle\n",
    "\n",
    "        ann = {\n",
    "            \"image_id\": \"videoframe\",\n",
    "            \"category_id\": 1,  # Persona\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "    # Agregar la leyenda al fotograma\n",
    "    status_text = f\"Steps: {steps} - Still: {'Yes' if is_still else 'No'} - Moving: {movement_direction} - Distance: {distance_direction}\"\n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Escribir el cuadro procesado en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Mostrar el cuadro procesado\n",
    "    cv2.imshow('BlazePose Result', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codigo de deteccion de pasos con direcciones Izq, Der, Adelante y Retroceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Función para convertir puntos clave de BlazePose a OpenPose\n",
    "\"\"\" def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 17\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = landmarks[1]  # Left eye\n",
    "    keypoints[2] = landmarks[4]  # Right eye\n",
    "    keypoints[3] = landmarks[7]  # Left ear\n",
    "    keypoints[4] = landmarks[8]  # Right ear\n",
    "    keypoints[5] = landmarks[11]  # Left shoulder\n",
    "    keypoints[6] = landmarks[12]  # Right shoulder\n",
    "    keypoints[7] = landmarks[13]  # Left elbow\n",
    "    keypoints[8] = landmarks[14]  # Right elbow\n",
    "    keypoints[9] = landmarks[15]  # Left wrist\n",
    "    keypoints[10] = landmarks[16]  # Right wrist\n",
    "    keypoints[11] = landmarks[23]  # Left hip\n",
    "    keypoints[12] = landmarks[24]  # Right hip\n",
    "    keypoints[13] = landmarks[25]  # Left knee\n",
    "    keypoints[14] = landmarks[26]  # Right knee\n",
    "    keypoints[15] = landmarks[27]  # Left ankle\n",
    "    keypoints[16] = landmarks[28]  # Right ankle\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])  # Fill with zeros if no point detected\n",
    "    return coco_keypoints\n",
    " \"\"\"\n",
    "\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 25\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = average_landmarks(landmarks[12], landmarks[11])  # Neck\n",
    "    keypoints[2] = landmarks[12]  # L shoulder\n",
    "    keypoints[3] = landmarks[14]  # L elbow \n",
    "    keypoints[4] = landmarks[16]  # L hand\n",
    "    keypoints[5] = landmarks[11]  # R shoulder\n",
    "    keypoints[6] = landmarks[13]  # R elbow\n",
    "    keypoints[7] = landmarks[15]  # R hand\n",
    "    keypoints[8] = average_landmarks(landmarks[24], landmarks[23])  # Hip central pelvis\n",
    "    keypoints[9] = landmarks[24]  # L hip\n",
    "    keypoints[10] = landmarks[26]  # L knee\n",
    "    keypoints[11] = landmarks[28]  # L ankle\n",
    "    keypoints[12] = landmarks[23]  # R hip\n",
    "    keypoints[13] = landmarks[25]  # R knee\n",
    "    keypoints[14] = landmarks[27]  # R ankle\n",
    "    keypoints[15] = average_landmarks_three(landmarks[5], landmarks[6], landmarks[4])  # Average of points\n",
    "    keypoints[16] = average_landmarks_three(landmarks[1], landmarks[2], landmarks[3])  # Average of points\n",
    "    keypoints[17] = landmarks[8]  # Custom point\n",
    "    keypoints[18] = landmarks[7]  # Custom point\n",
    "    keypoints[19] = landmarks[29]  # Custom point\n",
    "    keypoints[21] = landmarks[31]  # Custom point\n",
    "    keypoints[22] = landmarks[30]  # Custom point\n",
    "    keypoints[24] = landmarks[32]  # Custom point\n",
    "\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            if isinstance(point, dict):\n",
    "                coco_keypoints.extend([point['x'] * image_shape[1], point['y'] * image_shape[0], point['visibility']])\n",
    "            else:\n",
    "                coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])\n",
    "    return coco_keypoints\n",
    "\n",
    "# Función para dibujar puntos clave en la imagen\n",
    "def draw_coco_keypoints(image, keypoints):\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x, y, v = keypoints[i:i+3]\n",
    "        if v > 0:\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "# Función para dibujar el esqueleto\n",
    "def draw_coco_skeleton(image, keypoints, pairs):\n",
    "    for (start, end) in pairs:\n",
    "        x1, y1, v1 = keypoints[start*3:start*3+3]\n",
    "        x2, y2, v2 = keypoints[end*3:end*3+3]\n",
    "        if v1 > 0 and v2 > 0:\n",
    "            cv2.line(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "# Cargar el video\n",
    "#video_url = 'https://mcp-wildsense.s3.us-east-2.amazonaws.com/videos/7/2024-03-15/11_28_22-player9.mp4'\n",
    "video_url = 'https://mcp-wildsense.s3.us-east-2.amazonaws.com/videos/7/2024-03-15/10_51_54-player10.mp4'\n",
    "\n",
    "#cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "#cap = cv2.VideoCapture('./caminar2.mp4')\n",
    "cap = cv2.VideoCapture('./videoForDetection_v3.mp4')\n",
    "\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output2.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "blazepose_results = []\n",
    "previous_left_ankle = None\n",
    "previous_right_ankle = None\n",
    "steps = 0\n",
    "step_threshold = 70  # Umbral de distancia para considerar un paso (ajusta según sea necesario)\n",
    "still_threshold = 40  # Umbral de distancia para considerar que la persona está quieta\n",
    "still_frames = 0\n",
    "still_frames_threshold = 15  # Número de cuadros consecutivos para considerar que la persona está quieta\n",
    "is_still = False\n",
    "movement_direction = None\n",
    "movement_threshold = 10  # Umbral de movimiento en el eje X para considerar desplazamiento lateral\n",
    "distance_direction = None\n",
    "distance_threshold = 5  # Umbral de movimiento en el eje Y para considerar acercamiento/alejamiento\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Convertir a COCO keypoints\n",
    "        keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, frame.shape)\n",
    "        # Dibujar los puntos clave en formato COCO\n",
    "        draw_coco_keypoints(frame, keypoints)\n",
    "        # Dibujar esqueleto\n",
    "        skeleton_pairs = [\n",
    "            (0, 15), (15, 17), (0, 16), (16, 18), # Cabeza \n",
    "            (0, 1), #cuello\n",
    "            (1, 2), (2, 3), (3,4), #Brazo izquierdo\n",
    "            (1, 5), (6, 7), (5,6), # Brazos derechos\n",
    "            (1,8), #torso\n",
    "            (8, 9), (9, 10), (10, 11), (11, 22), (22, 24), (11, 24),  # Pierna izquierda\n",
    "            (8, 12), (12, 13), (13, 14), (14, 19), (19,21), (14, 21)  # Pierna derecha\n",
    "        ]\n",
    "        draw_coco_skeleton(frame, keypoints, skeleton_pairs)\n",
    "\n",
    "\n",
    "        # Obtener las posiciones de los tobillos\n",
    "        \"\"\" left_ankle = np.array([keypoints[33], keypoints[34]])  # Coordenadas del tobillo izquierdo\n",
    "        right_ankle = np.array([keypoints[39], keypoints[40]])  # Coordenadas del tobillo derecho\n",
    " \"\"\"\n",
    "        left_ankle = np.array([keypoints[11*3], keypoints[11*3+1]])  # Coordenadas del tobillo izquierdo\n",
    "        right_ankle = np.array([keypoints[14*3], keypoints[14*3+1]])   # Coordenadas del tobillo derecho\n",
    "\n",
    "\n",
    "        # Detectar pasos\n",
    "        if previous_left_ankle is not None and previous_right_ankle is not None:\n",
    "            left_distance = np.linalg.norm(left_ankle - previous_left_ankle)\n",
    "            right_distance = np.linalg.norm(right_ankle - previous_right_ankle)\n",
    "            if left_distance > step_threshold or right_distance > step_threshold:\n",
    "                steps += 1\n",
    "\n",
    "            # Detectar si la persona está quieta\n",
    "            if left_distance < still_threshold and right_distance < still_threshold:\n",
    "                still_frames += 1\n",
    "            else:\n",
    "                still_frames = 0\n",
    "\n",
    "            is_still = still_frames >= still_frames_threshold\n",
    "\n",
    "            # Detectar movimiento lateral\n",
    "            left_movement_x = left_ankle[0] - previous_left_ankle[0]\n",
    "            right_movement_x = right_ankle[0] - previous_right_ankle[0]\n",
    "\n",
    "            if abs(left_movement_x) > movement_threshold or abs(right_movement_x) > movement_threshold:\n",
    "                if left_movement_x > 0 and right_movement_x > 0:\n",
    "                    movement_direction = \"Right\"\n",
    "                elif left_movement_x < 0 and right_movement_x < 0:\n",
    "                    movement_direction = \"Left\"\n",
    "                else:\n",
    "                    movement_direction = \"Unknown\"\n",
    "            else:\n",
    "                movement_direction = \"Still\"\n",
    "\n",
    "            # Detectar acercamiento/alejamiento\n",
    "            left_movement_y = left_ankle[1] - previous_left_ankle[1]\n",
    "            right_movement_y = right_ankle[1] - previous_right_ankle[1]\n",
    "\n",
    "            if abs(left_movement_y) > distance_threshold or abs(right_movement_y) > distance_threshold:\n",
    "                if left_movement_y > 0 and right_movement_y > 0:\n",
    "                    distance_direction = \"Closer\"  # acerca\n",
    "                    steps += 1\n",
    "                elif left_movement_y < 0 and right_movement_y < 0:\n",
    "                    distance_direction = \"Farther\"  # aleja\n",
    "                    steps += 1\n",
    "                else:\n",
    "                    distance_direction = \"Unknown\"\n",
    "            else:\n",
    "                distance_direction = \"Stationary\"  # quiero en eje y\n",
    "\n",
    "        previous_left_ankle = left_ankle\n",
    "        previous_right_ankle = right_ankle\n",
    "\n",
    "        ann = {\n",
    "            \"image_id\": \"videoframe\",\n",
    "            \"category_id\": 1,  # Persona\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0  # Puedes ajustar el score según sea necesario\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "    # Agregar la leyenda al fotograma\n",
    "    status_text = f\"Steps: {steps} - Still: {'Yes' if is_still else 'No'} - Moving: {movement_direction} - Distance: {distance_direction}\"\n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Escribir el cuadro procesado en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Mostrar el cuadro procesado\n",
    "    cv2.imshow('BlazePose Result', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()\n",
    "\n",
    "# Guardar las anotaciones en un archivo JSON\n",
    "with open('blazepose_results2.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #graficar bounding boxes de las marcas del suelo\n",
    "\n",
    "# Open the video capture\n",
    "video_url = 'https://mcp-wildsense.s3.us-east-2.amazonaws.com/videos/7/2024-03-15/11_28_22-player9.mp4'\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "import cv2\n",
    "\n",
    "# Define area to mark (e.g., a bounding box)\n",
    "ax1, ay1, ax2, ay2 = 541, 604, 587, 630   # coordenadas de los circulos en el piso\n",
    "bx1, by1, bx2, by2 = 171, 525, 218, 550 \n",
    "cx1, cy1, cx2, cy2 = 162, 390, 197, 408\n",
    "dx1, dy1, dx2, dy2 = 923, 373, 953, 389 \n",
    "ex1, ey1, ex2, ey2 = 905, 504, 946, 526\n",
    "fx1, fy1, fx2, fy2 = 768, 299, 793, 318\n",
    "gx1, gy1, gx2, gy2 = 338, 305, 367, 319\n",
    "hx1, hy1, hx2, hy2 = 553, 381, 585, 396\n",
    "ix1, iy1, ix2, iy2 = 559, 279, 582, 289\n",
    "# Background subtraction (optional)\n",
    "# ... (implement background subtraction if needed)\n",
    "\n",
    "while True:\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "\n",
    "  # Check if frame is read correctly\n",
    "  if not ret:\n",
    "    print(\"Error: Unable to capture frame\")\n",
    "    break\n",
    "\n",
    "  # Apply background subtraction (if implemented)\n",
    "  # ... (apply background subtraction if used)\n",
    "\n",
    "  # Draw mark on the frame\n",
    "  cv2.rectangle(frame, (ax1, ay1), (ax2, ay2), (255, 0, 0), 2)  # Draw a red rectangle\n",
    "  cv2.rectangle(frame, (bx1, by1), (bx2, by2), (255, 0, 0), 2)\n",
    "  cv2.rectangle(frame, (cx1, cy1), (cx2, cy2), (255, 0, 0), 2)\n",
    "  cv2.rectangle(frame, (dx1, dy1), (dx2, dy2), (255, 0, 0), 2)\n",
    "  cv2.rectangle(frame, (ex1, ey1), (ex2, ey2), (255, 0, 0), 2)\n",
    "  cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (255, 0, 0), 2)\n",
    "  cv2.rectangle(frame, (gx1, gy1), (gx2, gy2), (255, 0, 0), 2)\n",
    "  cv2.rectangle(frame, (hx1, hy1), (hx2, hy2), (255, 0, 0), 2)\n",
    "  cv2.rectangle(frame, (ix1, iy1), (ix2, iy2), (255, 0, 0), 2)\n",
    "  # Display the resulting frame\n",
    "  cv2.imshow('Frame', frame)\n",
    "\n",
    "  # Exit if 'q' key is pressed\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "    break\n",
    "\n",
    "# Release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define area to mark (e.g., a bounding box)\n",
    "rectangles = [\n",
    "    (541, 604, 587, 630),  # Coordenadas de los círculos en el piso\n",
    "    (171, 525, 218, 550),\n",
    "    (162, 390, 197, 408),\n",
    "    (923, 373, 953, 389),\n",
    "    (905, 504, 946, 526),\n",
    "    (768, 299, 793, 311),\n",
    "    (338, 305, 367, 319),\n",
    "    (553, 381, 585, 396),\n",
    "    (559, 279, 582, 289)\n",
    "]\n",
    "\n",
    "def average_landmarks_three(landmark1, landmark2, landmark3):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x + landmark3.x) / 3,\n",
    "        'y': (landmark1.y + landmark2.y + landmark3.y) / 3,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility + landmark3.visibility) / 3\n",
    "    }\n",
    "\n",
    "def average_landmarks(landmark1, landmark2):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x) / 2,\n",
    "        'y': (landmark1.y + landmark2.y) / 2,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility) / 2\n",
    "    }\n",
    "\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 25\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = average_landmarks(landmarks[12], landmarks[11])  # Neck\n",
    "    keypoints[2] = landmarks[12]  # L shoulder\n",
    "    keypoints[3] = landmarks[14]  # L elbow \n",
    "    keypoints[4] = landmarks[16]  # L hand\n",
    "    keypoints[5] = landmarks[11]  # R shoulder\n",
    "    keypoints[6] = landmarks[13]  # R elbow\n",
    "    keypoints[7] = landmarks[15]  # R hand\n",
    "    keypoints[8] = average_landmarks(landmarks[24], landmarks[23])  # Hip central pelvis\n",
    "    keypoints[9] = landmarks[24]  # L hip\n",
    "    keypoints[10] = landmarks[26]  # L knee\n",
    "    keypoints[11] = landmarks[28]  # L ankle\n",
    "    keypoints[12] = landmarks[23]  # R hip\n",
    "    keypoints[13] = landmarks[25]  # R knee\n",
    "    keypoints[14] = landmarks[27]  # R ankle\n",
    "    keypoints[15] = average_landmarks_three(landmarks[5], landmarks[6], landmarks[4])  # Average of points\n",
    "    keypoints[16] = average_landmarks_three(landmarks[1], landmarks[2], landmarks[3])  # Average of points\n",
    "    keypoints[17] = landmarks[8]  # Custom point\n",
    "    keypoints[18] = landmarks[7]  # Custom point\n",
    "    keypoints[19] = landmarks[29]  # Custom point\n",
    "    keypoints[21] = landmarks[31]  # Custom point\n",
    "    keypoints[22] = landmarks[30]  # Custom point\n",
    "    keypoints[24] = landmarks[32]  # Custom point\n",
    "\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            if isinstance(point, dict):\n",
    "                coco_keypoints.extend([point['x'] * image_shape[1], point['y'] * image_shape[0], point['visibility']])\n",
    "            else:\n",
    "                coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])\n",
    "    return coco_keypoints\n",
    "\n",
    "def draw_coco_keypoints(image, keypoints):\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x, y, v = keypoints[i:i+3]\n",
    "        if v > 0:\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "            cv2.putText(image, str(i//3), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "def draw_coco_skeleton(image, keypoints, pairs):\n",
    "    for (start, end) in pairs:\n",
    "        x1, y1, v1 = keypoints[start*3:start*3+3]\n",
    "        x2, y2, v2 = keypoints[end*3:end*3+3]\n",
    "        if v1 > 0 and v2 > 0:\n",
    "            cv2.line(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "def point_in_rectangle(point, rect):\n",
    "    x, y = point\n",
    "    x1, y1, x2, y2 = rect\n",
    "    return x1 <= x <= x2 and y1 <= y <= y2\n",
    "\n",
    "# Cargar el video\n",
    "video_url = 'https://mcp-wildsense.s3.us-east-2.amazonaws.com/videos/7/2024-03-15/11_28_22-player9.mp4'\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "#cap = cv2.VideoCapture('./caminar2.mp4')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "blazepose_results = []\n",
    "previous_left_ankle = None\n",
    "previous_right_ankle = None\n",
    "steps = 0\n",
    "step_threshold = 30  # Umbral de distancia para considerar un paso (ajusta según sea necesario)\n",
    "still_threshold = 40  # Umbral de distancia para considerar que la persona está quieta\n",
    "still_frames = 0\n",
    "still_frames_threshold = 15  # Número de cuadros consecutivos para considerar que la persona está quieta\n",
    "is_still = False\n",
    "movement_direction = None\n",
    "movement_threshold = 10  # Umbral de movimiento en el eje X para considerar desplazamiento lateral\n",
    "distance_direction = None\n",
    "distance_threshold = 6  # Umbral de movimiento en el eje Y para considerar acercamiento/alejamiento\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    for rect in rectangles:\n",
    "        cv2.rectangle(frame, (rect[0], rect[1]), (rect[2], rect[3]), (255, 0, 0), 2)  # Draw rectangles\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        \n",
    "        keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, frame.shape)\n",
    "        \n",
    "        draw_coco_keypoints(frame, keypoints)\n",
    "        skeleton_pairs = [\n",
    "            (0, 15), (15, 17), (0, 16), (16, 18), # Cabeza \n",
    "            (0, 1), #cuello\n",
    "            (1, 2), (2, 3), (3,4), #Brazo izquierdo\n",
    "            (1, 5), (6, 7), (5,6), # Brazos derechos\n",
    "            (1,8), #torso\n",
    "            (8, 9), (9, 10), (10, 11), (11, 22), (22, 24),  # Pierna izquierda\n",
    "            (8, 12), (12, 13), (13, 14), (14, 19), (19,21)  # Pierna derecha\n",
    "        ]\n",
    "        draw_coco_skeleton(frame, keypoints, skeleton_pairs)\n",
    "\n",
    "        left_ankle = np.array([keypoints[35], keypoints[36]])  # Coordenadas del tobillo izquierdo\n",
    "        right_ankle = np.array([keypoints[39], keypoints[40]])  # Coordenadas del tobillo derecho\n",
    "\n",
    "        for rect in rectangles:\n",
    "            if point_in_rectangle(left_ankle, rect):\n",
    "                cv2.putText(frame, 'Left Ankle in Area', (rect[0], rect[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            if point_in_rectangle(right_ankle, rect):\n",
    "                cv2.putText(frame, 'Right Ankle in Area', (rect[0], rect[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if previous_left_ankle is not None and previous_right_ankle is not None:\n",
    "            left_distance = np.linalg.norm(left_ankle - previous_left_ankle)\n",
    "            right_distance = np.linalg.norm(right_ankle - previous_right_ankle)\n",
    "            if left_distance > step_threshold or right_distance > step_threshold:\n",
    "                steps += 1\n",
    "\n",
    "            if left_distance < still_threshold and right_distance < still_threshold:\n",
    "                still_frames += 1\n",
    "            else:\n",
    "                still_frames = 0\n",
    "\n",
    "            is_still = still_frames >= still_frames_threshold\n",
    "\n",
    "            left_movement_x = left_ankle[0] - previous_left_ankle[0]\n",
    "            right_movement_x = right_ankle[0] - previous_right_ankle[0]\n",
    "\n",
    "            if abs(left_movement_x) > movement_threshold or abs(right_movement_x) > movement_threshold:\n",
    "                if left_movement_x > 0 and right_movement_x > 0:\n",
    "                    movement_direction = \"Right\"\n",
    "                elif left_movement_x < 0 and right_movement_x < 0:\n",
    "                    movement_direction = \"Left\"\n",
    "                else:\n",
    "                    movement_direction = \"Unknown\"\n",
    "            else:\n",
    "                movement_direction = \"Still\"\n",
    "\n",
    "            left_movement_y = left_ankle[1] - previous_left_ankle[1]\n",
    "            right_movement_y = right_ankle[1] - previous_right_ankle[1]\n",
    "\n",
    "            if abs(left_movement_y) > distance_threshold or abs(right_movement_y) > distance_threshold:\n",
    "                if left_movement_y > 0 and right_movement_y > 0:\n",
    "                    distance_direction = \"Closer\"\n",
    "                    steps += 1\n",
    "                elif left_movement_y < 0 and right_movement_y < 0:\n",
    "                    distance_direction = \"Farther\"\n",
    "                    steps += 1\n",
    "                else:\n",
    "                    distance_direction = \"Unknown\"\n",
    "            else:\n",
    "                distance_direction = \"Stationary\"\n",
    "\n",
    "        previous_left_ankle = left_ankle\n",
    "        previous_right_ankle = right_ankle\n",
    "\n",
    "        ann = {\n",
    "            \"image_id\": \"videoframe\",\n",
    "            \"category_id\": 1,\n",
    "            \"keypoints\": keypoints,\n",
    "            \"score\": 1.0\n",
    "        }\n",
    "        blazepose_results.append(ann)\n",
    "\n",
    "    status_text = f\"Steps: {steps} - Still: {'Yes' if is_still else 'No'} - Moving: {movement_direction} - Distance: {distance_direction}\"\n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    out.write(frame)\n",
    "    cv2.imshow('BlazePose Result', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()\n",
    "\n",
    "with open('blazepose_results.json', 'w') as f:\n",
    "    json.dump(blazepose_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "33\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "\"\"\" #solo para tener la imagen \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define area to mark (e.g., a bounding box)\n",
    "rectangles = [\n",
    "    (541, 604, 587, 630),  # Coordenadas de los círculos en el piso\n",
    "    (171, 525, 218, 550),\n",
    "    (162, 390, 197, 408),\n",
    "    (923, 373, 953, 389),\n",
    "    (905, 504, 946, 526),\n",
    "    (768, 299, 793, 311),\n",
    "    (338, 305, 367, 319),\n",
    "    (553, 381, 585, 396),\n",
    "    (559, 279, 582, 289)\n",
    "]\n",
    "\n",
    "def average_landmarks_three(landmark1, landmark2, landmark3):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x + landmark3.x) / 3,\n",
    "        'y': (landmark1.y + landmark2.y + landmark3.y) / 3,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility + landmark3.visibility) / 3\n",
    "    }\n",
    "\n",
    "def average_landmarks(landmark1, landmark2):\n",
    "    return {\n",
    "        'x': (landmark1.x + landmark2.x) / 2,\n",
    "        'y': (landmark1.y + landmark2.y) / 2,\n",
    "        'visibility': (landmark1.visibility + landmark2.visibility) / 2\n",
    "    }\n",
    "\n",
    "def convert_blazepose_to_coco(landmarks, image_shape):\n",
    "    keypoints = [None] * 25\n",
    "    keypoints[0] = landmarks[0]  # Nose\n",
    "    keypoints[1] = average_landmarks(landmarks[12], landmarks[11])  # Neck\n",
    "    keypoints[2] = landmarks[12]  # L shoulder\n",
    "    keypoints[3] = landmarks[14]  # L elbow \n",
    "    keypoints[4] = landmarks[16]  # L hand\n",
    "    keypoints[5] = landmarks[11]  # R shoulder\n",
    "    keypoints[6] = landmarks[13]  # R elbow\n",
    "    keypoints[7] = landmarks[15]  # R hand\n",
    "    keypoints[8] = average_landmarks(landmarks[24], landmarks[23])  # Hip central pelvis\n",
    "    keypoints[9] = landmarks[24]  # L hip\n",
    "    keypoints[10] = landmarks[26]  # L knee\n",
    "    keypoints[11] = landmarks[28]  # L ankle\n",
    "    keypoints[12] = landmarks[23]  # R hip\n",
    "    keypoints[13] = landmarks[25]  # R knee\n",
    "    keypoints[14] = landmarks[27]  # R ankle\n",
    "    keypoints[15] = average_landmarks_three(landmarks[5], landmarks[6], landmarks[4])  # Average of points\n",
    "    keypoints[16] = average_landmarks_three(landmarks[1], landmarks[2], landmarks[3])  # Average of points\n",
    "    keypoints[17] = landmarks[8]  # Custom point\n",
    "    keypoints[18] = landmarks[7]  # Custom point\n",
    "    keypoints[19] = landmarks[29]  # Custom point\n",
    "    keypoints[21] = landmarks[31]  # Custom point\n",
    "    keypoints[22] = landmarks[30]  # Custom point\n",
    "    keypoints[24] = landmarks[32]  # Custom point\n",
    "    print(len(keypoints))\n",
    "    print(len(landmarks))\n",
    "    coco_keypoints = []\n",
    "    for point in keypoints:\n",
    "        if point is not None:\n",
    "            if isinstance(point, dict):\n",
    "                coco_keypoints.extend([point['x'] * image_shape[1], point['y'] * image_shape[0], point['visibility']])\n",
    "            else:\n",
    "                coco_keypoints.extend([point.x * image_shape[1], point.y * image_shape[0], point.visibility])\n",
    "        else:\n",
    "            coco_keypoints.extend([0, 0, 0])\n",
    "    print(len(coco_keypoints))\n",
    "    return coco_keypoints\n",
    "\n",
    "def draw_coco_keypoints(image, keypoints):\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x, y, v = keypoints[i:i+3]\n",
    "        if v > 0:\n",
    "            cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "            cv2.putText(image, str(i//3), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "def draw_coco_skeleton(image, keypoints, pairs):\n",
    "    for (start, end) in pairs:\n",
    "        x1, y1, v1 = keypoints[start*3:start*3+3]\n",
    "        x2, y2, v2 = keypoints[end*3:end*3+3]\n",
    "        if v1 > 0 and v2 > 0:\n",
    "            cv2.line(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "def point_in_rectangle(point, rect):\n",
    "    x, y = point\n",
    "    x1, y1, x2, y2 = rect\n",
    "    return x1 <= x <= x2 and y1 <= y <= y2\n",
    "\n",
    "# Cargar la imagen\n",
    "image_path = '../yolo/videoframe_0.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "for rect in rectangles:\n",
    "    cv2.rectangle(image, (rect[0], rect[1]), (rect[2], rect[3]), (255, 0, 0), 2)  # Draw rectangles\n",
    "\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "results = pose.process(image_rgb)\n",
    "\n",
    "if results.pose_landmarks:\n",
    "    keypoints = convert_blazepose_to_coco(results.pose_landmarks.landmark, image.shape)\n",
    "    draw_coco_keypoints(image, keypoints)\n",
    "    skeleton_pairs = [\n",
    "        (0, 15), (15, 17), (0, 16), (16, 18), # Cabeza \n",
    "        (0, 1), #cuello\n",
    "        (1, 2), (2, 3), (3,4), #Brazo izquierdo\n",
    "        (1, 5), (6, 7), (5,6), # Brazos derechos\n",
    "        (1,8), #torso\n",
    "        (8, 9), (9, 10), (10, 11), (11, 22), (22, 24),  # Pierna izquierda\n",
    "        (8, 12), (12, 13), (13, 14), (14, 19), (19,21)  # Pierna derecha\n",
    "    ]\n",
    "    draw_coco_skeleton(image, keypoints, skeleton_pairs)\n",
    "\n",
    "    left_ankle = np.array([keypoints[33], keypoints[34]])  # Coordenadas del tobillo izquierdo\n",
    "    right_ankle = np.array([keypoints[39], keypoints[40]])  # Coordenadas del tobillo derecho\n",
    "    \n",
    "    for rect in rectangles:\n",
    "        if point_in_rectangle(left_ankle, rect):\n",
    "            cv2.putText(image, 'Left Ankle in Area', (rect[0], rect[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        if point_in_rectangle(right_ankle, rect):\n",
    "            cv2.putText(image, 'Right Ankle in Area', (rect[0], rect[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('BlazePose Result', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()\n",
    "  \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
